{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from __future__ import division\n",
    "from sklearn.cluster import KMeans \n",
    "from numbers import Number\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sys, codecs, numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare test, train and validation for cleaned/light processed/not-filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original = pd.read_csv('dataset.csv', delimiter=',')\n",
    "cleaned = pd.read_csv('cleaned_tweet.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty = pd.isnull(cleaned).any(1).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5602                                          #atishoo W1 4 \n",
       "8871                                        &lt;3 alayellow \n",
       "11364                                         #shaundiviney \n",
       "547755                                         .....he's 19 \n",
       "548410                                             arhhghgh \n",
       "557244                                        ACAAAAAAAABOU \n",
       "561786                                       Ai...to dodÃ³i \n",
       "581991                                          Acaaaaaabou \n",
       "610232                                            chillaxin \n",
       "627736       ChÃ³ng máº·t, nhá»©c Ä‘áº§u --&gt; ngá»§ váº­y \n",
       "640717     Dáº¡o nÃ y náº¯ng nÃ³ng, Ä‘Ã´i lÃºc mÃ¬nh nÃ³n...\n",
       "652715                                       EW EW EW EW EW \n",
       "672799                           Ew ew ew ew ew ew ew ew... \n",
       "708101                                              F*ck it \n",
       "741877                                          He is...    \n",
       "761998                                          hauuuuuuuss \n",
       "773559                                        hi ho, hi ho. \n",
       "821550                                         Hi , Mï¿½low \n",
       "841245                                      I am chillaxin' \n",
       "902969                                           in my PJ's \n",
       "904563                                             im in IT \n",
       "907102                           Is at &quot;Up&quot; in 3D \n",
       "914685                                              Is I'll \n",
       "925890                                               is up. \n",
       "929461     Is...                                         ...\n",
       "943347                                             is in IT \n",
       "943415                                        is in my pj's \n",
       "950271                                          is so 50/50 \n",
       "954344                                        Is up at 2 am \n",
       "956403                                             is...... \n",
       "                                 ...                        \n",
       "989936                                   It's 9:01AM in L.A \n",
       "1057728                     Máº¡ng á»Ÿ quÃª cÃ¹i báº¯p quÃ¡ \n",
       "1079622                                 my throooaaaatttttt \n",
       "1092915                                       oucchh gusiku \n",
       "1095440     nÃ³ng tháº¿ nhá»‰, Ä‘Ã nh pháº£i Ä‘i bÆ¡i thÃ´i \n",
       "1149943                                   Ouuuuuucccccchhhh \n",
       "1150053                                     Ow. Ow. Ow..... \n",
       "1156079                                          OUUUUCHHHH \n",
       "1156388                                            ow ow ow \n",
       "1190867                                      se cerrï¿½ to' \n",
       "1235355              uhhhhhhhhhhhhhhhhurghhhhhhhhhhhhhhbish \n",
       "1250292    ThÃ´i rá»“i. Chuáº©n bá»‹ há»©ng chá»‹u háº­u ...\n",
       "1288466                                             up at 7 \n",
       "1288609                                            Up in 3d \n",
       "1288610                                           Up in 3-d \n",
       "1288611                                          Up in 3d = \n",
       "1288618                                           Up in 3D. \n",
       "1308166                                          up at 8 am \n",
       "1308594                                     UP is  qreeaatt \n",
       "1348677                                            yeeayyyy \n",
       "1360947                                          yooyoyyooo \n",
       "1366198               Ä‘Ã³i bá»¥ng rÃ¹i. mÃ  chÆ°a cÃ³ cÆ¡m \n",
       "1387221                                            Chilliin \n",
       "1398824                                             Ew 4/20 \n",
       "1434344                                         I am on MC. \n",
       "1473719                                          It's a 2:2 \n",
       "1506433                                         no on is on \n",
       "1520205    Pháº£i cháº³ng ta nÃªn Ä‘i uá»‘ng gÃ¬ Ä‘Ã³.......\n",
       "1522792                                            PS on FB \n",
       "1532812                                             So am I \n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original.SentimentText[empty]  #tweets that after filtration become nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original = original.drop(original.index[empty]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1578564 entries, 0 to 1578563\n",
      "Data columns (total 5 columns):\n",
      "index              1578564 non-null int64\n",
      "﻿ItemID            1578564 non-null int64\n",
      "Sentiment          1578564 non-null int64\n",
      "SentimentSource    1578564 non-null object\n",
      "SentimentText      1578564 non-null object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1578564 entries, 0 to 1578563\n",
      "Data columns (total 3 columns):\n",
      "index        1578564 non-null int64\n",
      "sentiment    1578564 non-null int64\n",
      "text         1578564 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 36.1+ MB\n"
     ]
    }
   ],
   "source": [
    "cleaned = cleaned.dropna().reset_index()\n",
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'follow friday exclamation user nicest girl ever'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.text.ix[5655]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FOLLOWFRIDAY! ----&gt; @LeezeArray &lt;---- nicest girl ever '"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original.SentimentText.ix[5655]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(790164, 788400)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = cleaned[cleaned.sentiment == 1]\n",
    "negative_tweets = cleaned[cleaned.sentiment == 0]\n",
    "len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788400 788400\n"
     ]
    }
   ],
   "source": [
    "positive_tweets = positive_tweets[:788400]\n",
    "negative_tweets = negative_tweets[:788400]\n",
    "print len(positive_tweets), len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'index', u'sentiment', u'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "base = pd.concat([positive_tweets, negative_tweets], axis=0, ignore_index=True)\n",
    "print base.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = base.index\n",
    "cleaned = cleaned.ix[l].reset_index()\n",
    "original = original.ix[l].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1277208,), (141912,), (157680,))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(base['text'], base['sentiment'], test_size=0.1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_val, y_train_val, test_size=0.1)\n",
    "X_train.shape, X_validation.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = X_train.index\n",
    "test = X_test.index\n",
    "val = X_validation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = cleaned.text.ix[train].values\n",
    "X_test = cleaned.text.ix[test].values\n",
    "X_validation = cleaned.text.ix[val].values\n",
    "\n",
    "y_train = cleaned.sentiment.ix[train].values\n",
    "y_test = cleaned.sentiment.ix[test].values\n",
    "y_validation = cleaned.sentiment.ix[val].values\n",
    "\n",
    "XX_train = original.SentimentText.ix[train].values\n",
    "XX_test  = original.SentimentText.ix[test].values\n",
    "XX_validation  = original.SentimentText[val].values\n",
    "\n",
    "yy_train = original.Sentiment[train]\n",
    "yy_test = original.Sentiment[test]\n",
    "yy_validation = original.Sentiment[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('duty with abi', '\\n', 'on duty with abi ')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[34] , '\\n', XX_test[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user dang were playing and was all night taking care sick baby and missed exclamation',\n",
       " '\\n',\n",
       " '@jordanknight Dang we were playing and I was up all night taking care of a sick baby and missed it! ')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[34] , '\\n', XX_train[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('user lot mind question always exclamation',\n",
       " '\\n',\n",
       " '@BlowhornOz A lot on my mind? Always! ')"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation[34] , '\\n', XX_validation[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1277208\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': X_train, 'sentiment': y_train})\n",
    "print(len(df))\n",
    "df.to_csv(\"archive/cleaned_tweet_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157680\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': X_test, 'sentiment': y_test})\n",
    "print(len(df))\n",
    "df.to_csv(\"archive/cleaned_tweet_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141912\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': X_validation, 'sentiment': y_validation})\n",
    "print(len(df))\n",
    "df.to_csv(\"archive/cleaned_tweet_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': XX_train, 'sentiment': yy_train})\n",
    "df.to_csv(\"archive/not_filtered_train.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame({'text': XX_test, 'sentiment': yy_test})\n",
    "df.to_csv(\"archive/not_filtered_test.csv\", index=False)\n",
    "\n",
    "df = pd.DataFrame({'text': XX_validation, 'sentiment': yy_validation})\n",
    "df.to_csv(\"archive/not_filtered_validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class autovivify_list(dict):\n",
    "        '''Pickleable class to replicate the functionality of collections.defaultdict'''\n",
    "        def __missing__(self, key):\n",
    "                value = self[key] = []\n",
    "                return value\n",
    "\n",
    "        def __add__(self, x):\n",
    "                '''Override addition for numeric types when self is empty'''\n",
    "                if not self and isinstance(x, Number):\n",
    "                        return x\n",
    "                raise ValueError\n",
    "\n",
    "        def __sub__(self, x):\n",
    "                '''Also provide subtraction method'''\n",
    "                if not self and isinstance(x, Number):\n",
    "                        return -1 * x\n",
    "                raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "        '''Read a GloVe array from sys.argv[1] and return its vectors and labels as arrays'''\n",
    "        numpy_arrays = []\n",
    "        labels_array = []\n",
    "        with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "            for c, r in enumerate(f):\n",
    "                sr = r.split()\n",
    "                labels_array.append(sr[0])\n",
    "                vec = numpy.array([float(i) for i in sr[1:]])\n",
    "                numpy_arrays.append(vec)\n",
    "    \n",
    "\n",
    "                    #if c == n_words:\n",
    "                    #        return numpy.array( numpy_arrays ), labels_array\n",
    "\n",
    "        return numpy.array( numpy_arrays ), labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_vector_file = 'glove.6B.200d.txt' \n",
    "df, labels_array  = build_word_vector_matrix(input_vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(df, lables):\n",
    "    words = lables\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "    \n",
    "    W =  df\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, vocab, ivocab = generate(df, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vec(W, vocab, ivocab, input_term, fail):\n",
    "    vecs = []\n",
    "    for idx, term in enumerate(input_term.split(' ')):\n",
    "        if term in vocab:\n",
    "            vecs.append(W[vocab[term], :])\n",
    "        else:\n",
    "            fail = fail + 1\n",
    "\n",
    "    size = len(vecs)\n",
    "    if size == 0:\n",
    "        vec_result = np.zeros((200,))\n",
    "    else:\n",
    "        vec_result = np.array(vecs).sum(axis = 0)/size\n",
    "    vec_norm = np.zeros(vec_result.shape)\n",
    "    d = (np.sum(vec_result ** 2,) ** (0.5))\n",
    "    vec_norm = (vec_result.T / d).T\n",
    "    return np.array(vec_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_calc(data):\n",
    "    fail = 0\n",
    "    t = datetime.datetime.now()\n",
    "    vectors = []\n",
    "    for tweet in data:\n",
    "        v = get_vec(W, vocab, ivocab, tweet,fail)\n",
    "        vectors.append(v)\n",
    "    print datetime.datetime.now() - t\n",
    "    print 'fail to find ', fail\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_db  =  pd.read_csv('archive/clean/cleaned_tweet_train.csv', delimiter=',')\n",
    "X_train = cl_db.text.values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157680,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_db_test  =  pd.read_csv('archive/clean/cleaned_tweet_test.csv', delimiter=',')\n",
    "X_test = cl_db_test.text.values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141912,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_db_val  =  pd.read_csv('archive/clean/cleaned_tweet_validation.csv', delimiter=',')\n",
    "X_validation = cl_db_val.text.values\n",
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:13.513994\n",
      "fail to find  0\n",
      "(1277208, 200)\n",
      "0:00:28.115516\n",
      "fail to find  0\n",
      "(157680, 200)\n",
      "0:00:10.955867\n",
      "fail to find  0\n",
      "(141912, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors = vec_calc(X_train)\n",
    "vectors = np.array(vectors)\n",
    "print vectors.shape\n",
    "f = open( \"archive/clean_gl_tw_tr_vec.pickle\", \"wb\" )\n",
    "pickle.dump(vectors, f)\n",
    "f.close()\n",
    "\n",
    "v_test = vec_calc(X_test)\n",
    "v_test = np.array(v_test)\n",
    "print v_test.shape\n",
    "f = open( \"archive/clean_gl_tw_tes_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_test, f)\n",
    "f.close()\n",
    "\n",
    "v_val = vec_calc(X_validation)\n",
    "v_val = np.array(v_val)\n",
    "print v_val.shape\n",
    "f = open( \"archive/clean_gl_tw_val_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_val, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1277208,)\n",
      "(157680,)\n",
      "(141912,)\n"
     ]
    }
   ],
   "source": [
    "cl_db  =  pd.read_csv('archive/lf/lf_tweet_train.csv', delimiter=',')\n",
    "XX_train = cl_db.text.values\n",
    "print XX_train.shape\n",
    "\n",
    "cl_db_test  =  pd.read_csv('archive/lf/lf_tweet_test.csv', delimiter=',')\n",
    "XX_test = cl_db_test.text.values\n",
    "print XX_test.shape\n",
    "\n",
    "\n",
    "cl_db_val  =  pd.read_csv('archive/lf/lf_tweet_val.csv', delimiter=',')\n",
    "XX_validation = cl_db_val.text.values\n",
    "print XX_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:46.985670\n",
      "fail to find  0\n",
      "(1277208, 200)\n",
      "0:00:20.665048\n",
      "fail to find  0\n",
      "(157680, 200)\n",
      "0:00:08.173685\n",
      "fail to find  0\n",
      "(141912, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors = vec_calc(XX_train)\n",
    "vectors = np.array(vectors)\n",
    "print vectors.shape\n",
    "f = open( \"archive/lf_gl_tw_tr_vec.pickle\", \"wb\" )\n",
    "pickle.dump(vectors, f)\n",
    "f.close()\n",
    "\n",
    "v_test = vec_calc(XX_test)\n",
    "v_test = np.array(v_test)\n",
    "print v_test.shape\n",
    "f = open( \"archive/lf_gl_tw_tes_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_test, f)\n",
    "f.close()\n",
    "\n",
    "v_val = vec_calc(X_validation)\n",
    "v_val = np.array(v_val)\n",
    "print v_val.shape\n",
    "f = open( \"archive/lf_gl_tw_val_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_val, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:14.934657\n",
      "fail to find  0\n",
      "(1277208, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors = vec_calc(X_train)\n",
    "vectors = np.array(vectors)\n",
    "print vectors.shape\n",
    "f = open( \"archive/clean_gl_wiki_tr_vec.pickle\", \"wb\" )\n",
    "pickle.dump(vectors, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True], dtype=bool)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = pickle.load(open('archive/clean_gl_wiki_tr_vec.pickle', 'rb'))\n",
    "v[199]==vectors[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:14.341741\n",
      "fail to find  0\n",
      "(157680, 200)\n"
     ]
    }
   ],
   "source": [
    "v_test = vec_calc(X_test)\n",
    "v_test = np.array(v_test)\n",
    "print v_test.shape\n",
    "f = open( \"archive/clean_gl_wiki_tes_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08.338756\n",
      "fail to find  0\n",
      "(141912, 200)\n"
     ]
    }
   ],
   "source": [
    "v_val = vec_calc(X_validation)\n",
    "v_val = np.array(v_val)\n",
    "print v_val.shape\n",
    "f = open( \"archive/clean_gl_wiki_val_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_val, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:17.200324\n",
      "fail to find  0\n",
      "(1277208,)\n"
     ]
    }
   ],
   "source": [
    "vectors = vec_calc(XX_train)\n",
    "vectors = np.array(vectors)\n",
    "print vectors.shape\n",
    "f = open( \"archive/lf_gl_wiki_tr_vec.pickle\", \"wb\" )\n",
    "pickle.dump(vectors, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open( \"archive/lf_gl_wiki_tr_vec.pickle\", \"wb\" )\n",
    "pickle.dump(vectors, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:22.486443\n",
      "fail to find  0\n",
      "(1277208, 200)\n"
     ]
    }
   ],
   "source": [
    "vectors = vec_calc(XX_train)\n",
    "vectors = np.array(vectors)\n",
    "print vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.349719\n",
      "fail to find  0\n",
      "(157680, 200)\n"
     ]
    }
   ],
   "source": [
    "v_test = vec_calc(XX_test)\n",
    "v_test = np.array(v_test)\n",
    "print v_test.shape\n",
    "f = open( \"archive/lf_gl_wiki_tes_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:08.405389\n",
      "fail to find  0\n",
      "(141912, 200)\n"
     ]
    }
   ],
   "source": [
    "v_val = vec_calc(X_validation)\n",
    "v_val = np.array(v_val)\n",
    "print v_val.shape\n",
    "f = open( \"archive/lf_gl_wiki_val_vec.pickle\", \"wb\" )\n",
    "pickle.dump(v_val, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = pd.read_csv('archive/not_filtered_test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment                     0\n",
       "text         no ISP on Monday. \n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.ix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157680 entries, 0 to 157679\n",
      "Data columns (total 2 columns):\n",
      "sentiment    157680 non-null int64\n",
      "text         157680 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = 157680\n",
    "XXX_test = []\n",
    "for i in xrange(l):\n",
    "    t = dset.text.ix[i] \n",
    "    text = light_process(t)\n",
    "    XXX_test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no isp on monday '"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XXX_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157680\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': XXX_test, 'sentiment': dset.sentiment})\n",
    "print(len(df))\n",
    "df.to_csv(\"archive/lf_tweet_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1277208 entries, 0 to 1277207\n",
      "Data columns (total 2 columns):\n",
      "sentiment    1277208 non-null int64\n",
      "text         1277208 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 19.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dset = pd.read_csv('archive/not_filtered_train.csv', delimiter=',')\n",
    "dset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 1277208\n",
    "XXX_train = []\n",
    "for i in xrange(l):\n",
    "    t = dset.text.ix[i] \n",
    "    text = light_process(t)\n",
    "    XXX_train.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': XXX_train, 'sentiment': dset.sentiment})\n",
    "df.to_csv(\"archive/lf_tweet_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141912 entries, 0 to 141911\n",
      "Data columns (total 2 columns):\n",
      "sentiment    141912 non-null int64\n",
      "text         141912 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dset = pd.read_csv('archive/not_filtered_validation.csv', delimiter=',')\n",
    "dset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 141912\n",
    "XXX_train = []\n",
    "for i in xrange(l):\n",
    "    t = dset.text.ix[i] \n",
    "    text = light_process(t)\n",
    "    XXX_train.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text': XXX_train, 'sentiment': dset.sentiment})\n",
    "df.to_csv(\"archive/lf_tweet_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwiki_train = pickle.load(open('archive/glove_wiki/clean/clean_gl_wiki_tr_vec.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwiki_tr_dset = pd.read_csv('archive/clean/cleaned_tweet_train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwiki_tr_lab = cwiki_tr_dset.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1277208,), (1277208, 200))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwiki_tr_lab.shape, cwiki_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 80000\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(cwiki_train[:train_size],cwiki_tr_lab[:train_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwiki_test = pickle.load(open('archive/glove_wiki/clean/clean_gl_wiki_tes_vec.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157680,), (157680, 200))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwiki_tes_dset = pd.read_csv('archive/clean/cleaned_tweet_test.csv', delimiter=',')\n",
    "cwiki_test_lab = cwiki_tes_dset.sentiment.values\n",
    "cwiki_test_lab.shape, cwiki_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tweet):\n",
    "    return clf.predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for vec in cwiki_test[:test_size]:\n",
    "    vec = vec.reshape(1,-1)\n",
    "    label = classify(vec)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i, l in enumerate(cwiki_test_lab[:test_size]):\n",
    "    if l == labels[i]:\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7115\n"
     ]
    }
   ],
   "source": [
    "print counter/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctw_train = pickle.load(open('archive/glove_twitter/clean/clean_gl_tw_tr_vec.pickle', 'rb'))\n",
    "ctw_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctw_tr_dset = pd.read_csv('archive/clean/cleaned_tweet_train.csv', delimiter=',')\n",
    "ctw_tr_lab = ctw_tr_dset.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctw_tr_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(ctw_train[:train_size],ctw_tr_lab[:train_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157680,), (157680, 200))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctw_test = pickle.load(open('archive/glove_twitter/clean/clean_gl_tw_tes_vec.pickle', 'rb'))\n",
    "ctw_test_dset = pd.read_csv('archive/clean/cleaned_tweet_test.csv', delimiter=',')\n",
    "ctw_test_lab = ctw_test_dset.sentiment.values\n",
    "ctw_test_lab.shape, ctw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for vec in ctw_test[:test_size]:\n",
    "    vec = vec.reshape(1,-1)\n",
    "    label = classify(vec)\n",
    "    labels.append(label)\n",
    "    \n",
    "counter = 0\n",
    "for i, l in enumerate(ctw_test_lab[:test_size]):\n",
    "    if l == labels[i]:\n",
    "        counter +=1\n",
    "print counter/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_train = pickle.load(open('archive/glove_twitter/lf/lf_gl_tw_tr_vec.pickle', 'rb'))\n",
    "lft_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lft_tr_dset = pd.read_csv('archive/lf/lf_tweet_train.csv', delimiter=',')\n",
    "lft_tr_lab = lft_tr_dset.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_tr_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n =  np.where(np.isnan(lft_train[:train_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = np.array(n)\n",
    "n = np.unique(n[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  855, 10148, 13625, 19709, 25558, 33375, 36026, 39831, 44481,\n",
       "       47679, 50059, 58155, 59421, 62027, 62103, 62424, 74119, 74775,\n",
       "       74926, 78275])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lft_train[n,:] = np.zeros((200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 200), (80000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_train[:train_size].shape, lft_tr_lab[:train_size].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(lft_train[:train_size],lft_tr_lab[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157680,), (157680, 200))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_test = pickle.load(open('archive/glove_twitter/lf/lf_gl_tw_tes_vec.pickle', 'rb'))\n",
    "lft_test_dset = pd.read_csv('archive/lf/lf_tweet_test.csv', delimiter=',')\n",
    "lft_test_lab = lft_test_dset.sentiment.values\n",
    "lft_test_lab.shape, lft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_test[:test_size].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "n =  np.where(np.isnan(lft_test[:test_size]))\n",
    "n = np.array(n)\n",
    "n = np.unique(n[0,:])\n",
    "print n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lft_test[n,:] = np.zeros((200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157680, 200)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lft_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for vec in lft_test[:test_size]:\n",
    "    vec = vec.reshape(1,-1)\n",
    "    label = classify(vec)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7311\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, l in enumerate(lft_test_lab[:test_size]):\n",
    "    if l == labels[i]:\n",
    "        counter +=1\n",
    "print counter/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208, 200)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_train = pickle.load(open('archive/lf_gl_wiki_tr_vec.pickle', 'rb'))\n",
    "lfw_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1277208,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_tr_dset = pd.read_csv('archive/lf/lf_tweet_train.csv', delimiter=',')\n",
    "lfw_tr_lab = lfw_tr_dset.sentiment.values\n",
    "lfw_tr_lab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39,)\n"
     ]
    }
   ],
   "source": [
    "n =  np.where(np.isnan(lfw_train[:train_size]))\n",
    "n = np.array(n)\n",
    "n = np.unique(n[0,:])\n",
    "print n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfw_train[n,:] = np.zeros((200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 200), (80000,))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_train[:train_size].shape,lfw_tr_lab[:train_size].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(lfw_train[:train_size],lfw_tr_lab[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157680,), (157680, 200))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfw_test = pickle.load(open('archive/lf_gl_wiki_tes_vec.pickle', 'rb'))\n",
    "lfw_test_dset = pd.read_csv('archive/lf/lf_tweet_test.csv', delimiter=',')\n",
    "lfw_test_lab = lfw_test_dset.sentiment.values\n",
    "lfw_test_lab.shape, lfw_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "n =  np.where(np.isnan(lfw_test[:test_size]))\n",
    "n = np.array(n)\n",
    "n = np.unique(n[0,:])\n",
    "print n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lfw_test[n,:] = np.zeros((200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for vec in lfw_test[:test_size]:\n",
    "    vec = vec.reshape(1,-1)\n",
    "    label = classify(vec)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7051\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, l in enumerate(lfw_test_lab[:test_size]):\n",
    "    if l == labels[i]:\n",
    "        counter +=1\n",
    "print counter/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5761,)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf = np.array(wtf)\n",
    "wtf = np.unique(wtf)\n",
    "wtf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aabangan', 'aaeeaa', 'aafekm', 'aagenealogy',\n",
       "       'aahahahahahahahahahahahahahahahah', 'aahahahha', 'aahmhm',\n",
       "       'aaiihh', 'aalinakirche', 'aaliyahlove', 'aamaustin', 'aandhi',\n",
       "       'aanswwe', 'aaooww', 'aargghh', 'aarghh', 'aarrgghh', 'aarrgh',\n",
       "       'aawurl', 'aawwhh', 'abangmu', 'abcfamily',\n",
       "       'abc\\xc3\\xaf\\xc2\\xbf\\xc2\\xbds', 'abduzeedo', 'abduzzeedo',\n",
       "       'abfahrt', 'abigaillyy', 'abondigas', 'abrazos', 'abseloutly',\n",
       "       'aburriendomee', 'ab\\xc3\\x83\\xc2\\xaeme', 'acadeaua',\n",
       "       'accesshollywood', 'aceburpeeshow', 'aceisancient', 'aceleasi',\n",
       "       'acenetc', 'aceybear', 'aclientsplatform', 'acrisoare',\n",
       "       'acrylicana', 'actionfigure', 'activisionsucks', 'actofgreen',\n",
       "       'acupunc', 'adamcheeserosie', 'adamgazm', 'adamisarockstar',\n",
       "       'adamlambert', 'adamliveson', 'addictivetvshows', 'adiabatman',\n",
       "       'adictaa', 'adinarayanap', 'adt\\xc3\\xaf\\xc2\\xbf\\xc2\\xbd',\n",
       "       'adultbaby', 'adultswim', 'adverblog', 'adweebs', 'afazenda',\n",
       "       'affiliconil', 'affright', 'afterprom', 'aftersun', 'afterthefact',\n",
       "       'agarrar', 'ageplay', 'agreegree', 'agwee', 'ahahah', 'ahahahaha',\n",
       "       'ahahahahah', 'ahahahahaha', 'ahahahahahaa', 'ahahahahahah',\n",
       "       'ahahahahahaha', 'ahahahahahahahaha', 'ahahahhaah', 'ahhaahaha',\n",
       "       'ahhpink', 'ahmadin', 'ahmadinedschad', 'ahoova', 'ahsuhasuha',\n",
       "       'ah\\xc3\\x83\\xc2\\xad', 'ah\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6sorry',\n",
       "       'aikoihin', 'aircond', 'aircondition', 'airfrace', 'airfrance',\n",
       "       'airmaster', 'airmood', 'airtels', 'aishazam', 'aiyerhh',\n",
       "       'ajaedandridge', 'ajajajajajaj', 'ajjajaj'], \n",
       "      dtype='|S134')"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aagenealogy\n",
      "activisionsucks\n",
      "affright\n",
      "angelically\n",
      "armhole\n",
      "bondman\n",
      "butchano\n",
      "caleche\n",
      "carious\n",
      "cassadinkle\n",
      "castigatorul\n",
      "chalkpit\n",
      "chigoe\n",
      "chilblain\n",
      "churrcchh\n",
      "cocksucker\n",
      "confuzzled\n",
      "crackalackin\n",
      "cyberhallelujah\n",
      "danceordiewithavengeance\n",
      "dayquil\n",
      "deevvoo\n",
      "duvelisdendrank\n",
      "ecomonday\n",
      "ferrarii\n",
      "fichu\n",
      "frappachins\n",
      "frappucino\n",
      "greenflys\n",
      "gtretweet\n",
      "halfwitt\n",
      "hugglesxx\n",
      "imisscath\n",
      "inaninyonga\n",
      "incise\n",
      "invitanï¿½\n",
      "laceylynnwilliams\n",
      "ladyloveslondonatgmaildotcom\n",
      "madadapa\n",
      "masterbating\n",
      "mayblowyourmind\n",
      "membra\n",
      "memoribillia\n",
      "metaphysis\n",
      "motorycle\n",
      "mozmemories\n",
      "multumesc\n",
      "mygyver\n",
      "myspizzle\n",
      "octopods\n",
      "officialbowwow\n",
      "pearshaped\n",
      "pelforth\n",
      "placethere\n",
      "playfellow\n",
      "pneumonoultramicroscopicsilicovolcanoconiosis\n",
      "pubes\n",
      "puttee\n",
      "reeyasabellina\n",
      "skyblue\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-539-5ba75933ac35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwordlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInitializeWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwtf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParseWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-ac9189dbe03b>\u001b[0m in \u001b[0;36mParseWord\u001b[0;34m(term, wordlist)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFindWord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-498-ac9189dbe03b>\u001b[0m in \u001b[0;36mFindWord\u001b[0;34m(token, wordlist)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aba',\n",
       " 'age',\n",
       " 'aha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'haha',\n",
       " 'aha',\n",
       " 'yah',\n",
       " 'love',\n",
       " 'ama',\n",
       " 'tin',\n",
       " 'and',\n",
       " 'aba',\n",
       " 'abc',\n",
       " 'family',\n",
       " 'abc',\n",
       " 'zee',\n",
       " 'zee',\n",
       " 'abigail',\n",
       " 'dig',\n",
       " 'abra',\n",
       " 'lout',\n",
       " 'rien',\n",
       " 'dome',\n",
       " 'ade',\n",
       " 'access',\n",
       " 'holly',\n",
       " 'wood',\n",
       " 'ace',\n",
       " 'burp',\n",
       " 'how',\n",
       " 'ace',\n",
       " 'ancient',\n",
       " 'ace',\n",
       " 'leas',\n",
       " 'ace',\n",
       " 'net',\n",
       " 'ace',\n",
       " 'bear',\n",
       " 'lien',\n",
       " 'splat',\n",
       " 'form',\n",
       " 'acris',\n",
       " 'oar',\n",
       " 'acrylic',\n",
       " 'ana',\n",
       " 'action',\n",
       " 'figure',\n",
       " 'vision',\n",
       " 'suck',\n",
       " 'act',\n",
       " 'green',\n",
       " 'acu',\n",
       " 'pun',\n",
       " 'adam',\n",
       " 'cheese',\n",
       " 'adam',\n",
       " 'adam',\n",
       " 'adam',\n",
       " 'lambert',\n",
       " 'adam',\n",
       " 'lives',\n",
       " 'addictive',\n",
       " 'show',\n",
       " 'aba',\n",
       " 'man',\n",
       " 'ara',\n",
       " 'yana',\n",
       " 'adult',\n",
       " 'baby',\n",
       " 'adult',\n",
       " 'swim',\n",
       " 'adverb',\n",
       " 'log',\n",
       " 'wee',\n",
       " 'faze',\n",
       " 'icon',\n",
       " 'after',\n",
       " 'prom',\n",
       " 'after',\n",
       " 'sun',\n",
       " 'after',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'agar',\n",
       " 'age',\n",
       " 'play',\n",
       " 'agree',\n",
       " 'gre',\n",
       " 'wee',\n",
       " 'aha',\n",
       " 'aha',\n",
       " 'haha',\n",
       " 'aha',\n",
       " 'haha',\n",
       " 'aha']"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5833,)"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0116', '0546', '0631', '09011', '108370987500', '11327',\n",
       "       '1234567890', '12800', '13341015518', '1337357', '13850', '14517',\n",
       "       '1586789', '18312', '184948', '1898553', '19627', '2013212491',\n",
       "       '2174', '22850', '2669', '2790', '2807', '3175228858', '32509',\n",
       "       '361733', '4291', '4357', '43829', '4434156058', '4650', '4980',\n",
       "       '5015', '5183', '5278020', '5319', '53344', '533939',\n",
       "       '5346543212121214572', '5610', '5678', '6034385620', '61423682984',\n",
       "       '6156183900', '6173', '621621621', '6280', '6380', '6597881990',\n",
       "       '6958', '708836', '723897', '74892', '7837', '7897389472', '8193',\n",
       "       '8199', '8330', '8537', '85455', '864290', '8675309',\n",
       "       '89374789329478', '90028', '9157', '9203427290', '923400', '92683',\n",
       "       '9401', '9435', '9496808819', '9553', 'aabangan', 'aaeeaa',\n",
       "       'aafekm', 'aagenealogy', 'aahahahahahahahahahahahahahahahah',\n",
       "       'aahahahha', 'aahmhm', 'aaiihh', 'aalinakirche', 'aaliyahlove',\n",
       "       'aamaustin', 'aandhi', 'aanswwe', 'aaooww', 'aargghh', 'aarghh',\n",
       "       'aarrgghh', 'aarrgh', 'aawurl', 'aawwhh', 'abangmu', 'abcfamily',\n",
       "       'abc\\xc3\\xaf\\xc2\\xbf\\xc2\\xbds', 'abduzeedo', 'abduzzeedo',\n",
       "       'abfahrt', 'abigaillyy', 'abondigas'], \n",
       "      dtype='|S134')"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def InitializeWords():\n",
    "    wordlist = 'dic.txt' # A file containing common english words\n",
    "    content = None\n",
    "    with open(wordlist) as f:\n",
    "        content = f.readlines()\n",
    "    return [word.rstrip('\\n') for word in content]\n",
    "\n",
    "\n",
    "def ParseWord(term, wordlist):\n",
    "    words = []\n",
    "    # Remove hashtag, split by dash\n",
    "    word = FindWord(term, wordlist)    \n",
    "    while word != None and len(term) > 0:\n",
    "        words += [word]            \n",
    "        if len(term) == len(word): # Special case for when eating rest of word\n",
    "            break\n",
    "        term = term[len(word):]\n",
    "        word = FindWord(term, wordlist)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def FindWord(token, wordlist):\n",
    "    i = len(token) + 1\n",
    "    while i > 1:\n",
    "        i -= 1\n",
    "        if token[:i] in wordlist:\n",
    "            return token[:i]\n",
    "    return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abra z os\n",
      "ab se lout ly\n",
      "ab ur rien dome e\n",
      "ab\n",
      "ac ade au a\n",
      "access holly wood\n",
      "ace burp e es how\n",
      "ace is ancient\n",
      "ace leas i\n",
      "ace net c\n",
      "ace y bear\n",
      "ac lien t splat form\n",
      "acris oar e\n",
      "acrylic ana\n",
      "action figure\n",
      "acti vision suck s\n",
      "act of green\n",
      "acu pun c\n",
      "adam cheese ro si e\n",
      "adam ga z m\n",
      "adam is ar o c\n",
      "adam lambert\n",
      "adam lives on\n",
      "addictive t v show s\n",
      "ad i aba t man\n",
      "ad i c ta a\n",
      "ad in ara yana p\n",
      "ad t\n",
      "adult baby\n",
      "adult swim\n",
      "adverb log\n",
      "ad wee b s\n",
      "a faze nd a\n",
      "a ff il icon il\n",
      "affright\n",
      "after prom\n",
      "after sun\n",
      "after the fact\n",
      "agar ra r\n",
      "age play\n",
      "agree gre e\n",
      "a g wee\n",
      "aha ha\n",
      "aha haha ha\n",
      "aha haha ha\n",
      "aha haha haha\n",
      "aha haha haha a\n",
      "aha haha haha\n",
      "aha haha haha ha\n",
      "aha haha haha haha ha\n",
      "aha ha\n",
      "ah ha aha ha\n",
      "ah\n",
      "ah mad in\n",
      "ah mad in ed sc had\n",
      "ah o o va\n",
      "ah su has u ha\n",
      "ah\n",
      "ah\n",
      "a i\n",
      "air con d\n",
      "air condition\n",
      "air fr ace\n",
      "air france\n",
      "air master\n",
      "air mood\n",
      "air tel s\n",
      "a is ha za m\n",
      "a i ye rh\n",
      "a ja ed and ridge\n",
      "a ja ja ja ja ja\n",
      "a\n",
      "a kh i r ny a\n",
      "a kj a va\n",
      "a komi sm o\n",
      "a\n",
      "a\n",
      "a kw tj wk l\n",
      "ala nc ar r\n",
      "ala nis sarcastic\n",
      "alba n na ch\n",
      "ale nina\n",
      "ale rg i c\n",
      "alex ins\n",
      "alex ist ta\n",
      "alex johnson\n",
      "al g u\n",
      "al it ho s\n",
      "allah pundit\n",
      "all blacken t\n",
      "all ie e\n",
      "all int hemi nd\n",
      "all is hi a\n",
      "all night er\n",
      "all time low\n",
      "all t s\n",
      "almond it o\n",
      "al mo\n",
      "al mu er z o\n",
      "aloha friday\n"
     ]
    }
   ],
   "source": [
    "wordlist = InitializeWords()\n",
    "for word in wtf[100:200]:\n",
    "    w = ParseWord(word, wordlist)\n",
    "    print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('tw-abr-dict.json') as data_file:    \n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6239,)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0546', '0631', '09011', '093m3', '0o0o0oh', '100gb', '100miles',\n",
       "       '100miletraining', '100ms', '100pushups', '100times', '100v',\n",
       "       '10celcius', '10h30m', '10mins', '10yds', '10yearsofenema', '10yrs',\n",
       "       '11327', '120gb', '120k', '12800', '1280x800', '128gbs', '128pc',\n",
       "       '12\\xc3\\x82\\xc2\\xb0c', '130am', '13341015518', '1337357', '1337th',\n",
       "       '13850', '13lbs', '13musiconline', '13yrold', '140charfail',\n",
       "       '140conf', '140smiles', '140tc', '1500k', '150mb', '1580px',\n",
       "       '1586789', '15hrs', '15yrs', '160gb', '16mbps', '16thru21', '16wks',\n",
       "       '18312', '184948', '1898553', '18h30m', '18hrs', '18yr', '1900txts',\n",
       "       '195bucks', '19627', '1stds10', '1weikert', '200ad', '2013212491',\n",
       "       '20gb', '20lbs', '20mins', '20x5', '20yrs', '2174', '21yr', '22850',\n",
       "       '232\\xc3\\xaf\\xc2\\xbf\\xc2\\xbd', '234am', '23mins', '250pp',\n",
       "       '256kbps', '25mins', '260k', '2669', '2790', '27hrs', '2807',\n",
       "       '28kbps', '2cos2x', '2getha', '2junxion', '2morra', '2mozz',\n",
       "       '2mrrw', '2saymyspacetwitters', '2stacey', '2wks2go',\n",
       "       '2\\xc3\\xab\\xe2\\x80\\xb9\\xc2\\xac\\xc3\\xab', '30lbs', '30minmeditation',\n",
       "       '30mins', '30secondstomars', '30secondstomarsiscoming', '30stm',\n",
       "       '30stmglyphs', '3175228858', '31wsop'], \n",
       "      dtype='|S134')"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13890,)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf.shape #without spliting hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['websiite',\n",
       " 'wallpaperdont',\n",
       " 'quotsighquot',\n",
       " 'quotlord',\n",
       " 'ahahah',\n",
       " 'mollybut',\n",
       " 'unfollowdiddy',\n",
       " 'quotfor',\n",
       " 'namequot',\n",
       " 'namegood',\n",
       " 'whyquot',\n",
       " 'snowhite',\n",
       " 'asracist',\n",
       " 'russfail',\n",
       " 'squarespace',\n",
       " 'hahaill',\n",
       " 'robotpickuplines',\n",
       " 'thecsperspectiveuser',\n",
       " 'isembeck',\n",
       " 'hadn',\n",
       " 'blaarhg',\n",
       " 'raingodd',\n",
       " 'quotthinnerquot',\n",
       " 'alliee',\n",
       " 'arghghgghoh',\n",
       " 'shakespearesaturday',\n",
       " 'quotwatching',\n",
       " 'elephantsquot',\n",
       " 'puttee',\n",
       " 'espesh',\n",
       " 'followfriday',\n",
       " 'hehehedead',\n",
       " 'iranelection',\n",
       " 'voc\\xc3\\xaf\\xc2\\xbf\\xc2\\xbds',\n",
       " 'quottwittandoquot',\n",
       " 'a\\xc3\\xaf\\xc2\\xbf\\xc2\\xbd',\n",
       " 'hahahahah',\n",
       " 'prreeyty',\n",
       " '\\xc3\\x82\\xc2\\xb4re',\n",
       " 'solutionproject',\n",
       " 'babymommaa',\n",
       " 'eitherfail',\n",
       " 'qualifyingrace',\n",
       " 'quotyou',\n",
       " 'youquotand',\n",
       " 'girliegirl',\n",
       " 'cutegirl',\n",
       " 'funkygirl',\n",
       " 'followfriday',\n",
       " 'woteva',\n",
       " 'sherdinator',\n",
       " '\\xc3\\xaf\\xc2\\xbf\\xc2\\xbd',\n",
       " 'textswtf',\n",
       " 'hmmlooking',\n",
       " 'itdamnkids',\n",
       " 'quotsettingsquot',\n",
       " 'keybindings',\n",
       " 'thingsihate',\n",
       " 'forevaahh',\n",
       " 'followfridayprrs',\n",
       " 'babyintheoven',\n",
       " 'calendimaggio',\n",
       " 'booiiwwaayy',\n",
       " 'endquot',\n",
       " 'demitoo',\n",
       " 'scotfree',\n",
       " 'gbquot',\n",
       " 'basementbesides',\n",
       " 'ahahah',\n",
       " 'bdemarco',\n",
       " 'daniellei',\n",
       " 'concarolinas',\n",
       " 'bananaweizen',\n",
       " 'quotknowingquot',\n",
       " 'quotanecdotequot',\n",
       " 'quotdataquot',\n",
       " 'dannygokey',\n",
       " 'tizz\\xc3\\xa2\\xe2\\x84\\xa2\\xc2\\xa5',\n",
       " 'lionessrawr',\n",
       " 'quotjohnnyquot',\n",
       " 'quotfquot',\n",
       " 'inaperfectworld',\n",
       " 'lim\\xc3\\x83\\xc2\\xb3n',\n",
       " 'qu\\xc3\\x83\\xc2\\xa9',\n",
       " 's\\xc3\\x83\\xc2\\xa9',\n",
       " 's\\xc3\\x83\\xc2\\xa9',\n",
       " 'hxhxkamis',\n",
       " 'ngumpul',\n",
       " 'berencana',\n",
       " 'int\\xc3\\x83\\xc2\\xa9ressant',\n",
       " 'xenserver',\n",
       " 'rubbishness',\n",
       " 'followfirday',\n",
       " '\\xc3\\xa0\\xc2\\xb8\\xe2\\x80\\x9e\\xc3\\xa0\\xc2\\xb8\\xc2\\xa3\\xc3\\xa0\\xc2\\xb8\\xc2\\xb1\\xc3\\xa0\\xc2\\xb8\\xc5\\xa1\\xc3\\xa0\\xc2\\xb8\\xc5\\x93\\xc3\\xa0\\xc2\\xb8\\xc2\\xa1',\n",
       " 'stalkerlicious',\n",
       " 'quotuser',\n",
       " 'facehunter',\n",
       " 'itsuckswhen',\n",
       " 'ahahah',\n",
       " 'lastnight']"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26059,)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtf.shape #old. Do not touch! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = np.array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 50)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(tweets,y_train) \n",
    "\n",
    "def classify(tweet):\n",
    "    return clf.predict(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for tweet in X_test:\n",
    "    tweet = processTweet(tweet)\n",
    "    vec = get_vec(W, vocab, ivocab,tweet).reshape(1,-1)\n",
    "    if vec.shape != (1,50):\n",
    "        vec = np.zeros((1,50))\n",
    "    label = classify(vec)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i, l in enumerate(y_test):\n",
    "    if l == labels[i]:\n",
    "        counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6686\n"
     ]
    }
   ],
   "source": [
    "print counter/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define all useful functions\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "        \n",
    "# define yout train and accuracy functions first         \n",
    "def train_nn(X_train, y_train, train_f, accuracy_f, num_epochs = 20, batch_size = 100, verbose = False):\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train,batch_size):\n",
    "            inputs, targets = batch\n",
    "            targets = targets.reshape(batch_size,1)\n",
    "            train_err_batch, train_acc_batch = train_f(inputs, targets)\n",
    "            train_err += train_err_batch\n",
    "            train_acc += train_acc_batch \n",
    "            train_batches += 1\n",
    "\n",
    "#         # And a full pass over the validation data:\n",
    "#         val_acc = 0\n",
    "#         val_batches = 0\n",
    "#         for batch in iterate_minibatches(X_val, y_val, batch_size):\n",
    "#             inputs, targets = batch\n",
    "#             val_acc += accuracy_f(inputs, targets)\n",
    "#             val_batches += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "            print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "                train_acc / train_batches * 100))\n",
    "#             print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "#                 val_acc / val_batches * 100))\n",
    "        \n",
    "    train_accuracy = train_acc / train_batches * 100\n",
    "#     validation_accuracy = val_acc / val_batches * 100\n",
    "    \n",
    "    validation_accuracy = 0\n",
    "    \n",
    "    return train_accuracy, validation_accuracy\n",
    "\n",
    "\n",
    "def test_nn(X_test, y_test, accuracy_f, batch_size = 100, verbose = False):\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    \n",
    "    for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "        inputs, targets = batch\n",
    "        targets = targets.reshape(batch_size,1)\n",
    "        acc = accuracy_f(inputs, targets)\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "        \n",
    "    test_accuracy = test_acc / test_batches * 100\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "def conv_nn(input_X, input_shape, dropout = False):\n",
    "    net = lasagne.layers.InputLayer(shape = input_shape, input_var=input_X)\n",
    "    \n",
    "    nets = []\n",
    "    #hidden\n",
    "    for filter_size in filter_sizes:\n",
    "        temp_net = lasagne.layers.Conv1DLayer(net, num_filters=num_filters, filter_size=filter_size)\n",
    "        temp_net = lasagne.layers.MaxPool1DLayer(net, pool_size=2)\n",
    "        \n",
    "        nets.append(temp_net)\n",
    "        \n",
    "    # concat layers for different filter sizes     \n",
    "    net = lasagne.layers.concat(nets) \n",
    "    \n",
    "    net = lasagne.layers.DenseLayer(net, num_units = 50,\n",
    "                                         nonlinearity = lasagne.nonlinearities.rectify)\n",
    "    # output\n",
    "    net = lasagne.layers.DenseLayer(net, num_units = 1,\n",
    "                                         nonlinearity = lasagne.nonlinearities.sigmoid,\n",
    "                                         name='output')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "n_dim = 50\n",
    "num_filters = 50\n",
    "filter_sizes = [3,4,5]\n",
    "input_shape = [None, 1, n_dim]\n",
    "\n",
    "input_X = T.tensor3('X', dtype=\"float32\")\n",
    "input_y = T.matrix('y', dtype=\"int32\")\n",
    "target_y = T.matrix(\"target Y\",dtype='int32')\n",
    "\n",
    "\n",
    "train_v = X_train\n",
    "test_v = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "num_epochs = 400   #amount of passes through the data\n",
    "batch_size = 400   #number of samples processed at each function call\n",
    "\n",
    "net = conv_nn(input_X, input_shape=input_shape)\n",
    "\n",
    "# basic training loop \n",
    "\n",
    "#network prediction (theano-transformation)\n",
    "y_predicted = lasagne.layers.get_output(net)\n",
    "\n",
    "#all network weights (shared variables)\n",
    "all_weights = lasagne.layers.get_all_params(net, trainable = True)\n",
    "\n",
    "#binary crossentropy as loss function\n",
    "loss = lasagne.objectives.binary_crossentropy(y_predicted, target_y).mean()\n",
    "\n",
    "#prediction accuracy\n",
    "accuracy = lasagne.objectives.binary_accuracy(y_predicted, target_y).mean()\n",
    "\n",
    "#gradient computation + weights update\n",
    "updates = lasagne.updates.rmsprop(loss, all_weights, learning_rate=0.001)\n",
    "\n",
    "#function that computes loss and updates weights\n",
    "train = theano.function([input_X, target_y],[loss, accuracy], updates = updates, allow_input_downcast=True)\n",
    "\n",
    "#function that just computes accuracy\n",
    "accuracy = theano.function([input_X,target_y], accuracy, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for t in X_train[:100]:\n",
    "    print t\n",
    "    t = processTweet(t)\n",
    "    print t\n",
    "    v = get_vec(W, vocab, ivocab, t).reshape(1,-1)\n",
    "    if v.shape != (1,50):\n",
    "        v = np.zeros((1,50))\n",
    "    tweets.append(np.array(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = np.array(tweets)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_tweet = []\n",
    "\n",
    "for t in X_test:\n",
    "    t = processTweet(t)\n",
    "    v = get_vec(W, vocab, ivocab, t).reshape(1,-1)\n",
    "    if v.shape != (1,50):\n",
    "        v = np.zeros((1,50))\n",
    "    test_tweet.append(np.array(v))\n",
    "\n",
    "test_tweet = np.array(test_tweet)\n",
    "test_tweet.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_accuracy, validation_accuracy = train_nn(tweets, y_train, train, accuracy,\n",
    "                                               num_epochs = num_epochs, \n",
    "                                               batch_size = batch_size, verbose = True)\n",
    "\n",
    "test_accuracy = test_nn(test_tweet, y_test, accuracy, batch_size = 400, verbose = False)\n",
    "\n",
    "print(\"conv nn \")\n",
    "print(\"  train accuracy:\", train_accuracy)\n",
    "# print(\"  validation accuracy:\", validation_accuracy)\n",
    "print(\"  test accuracy:\", test_accuracy)\n",
    "print(\"____________________________________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
