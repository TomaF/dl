{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from __future__ import division\n",
    "from sklearn.cluster import KMeans \n",
    "from numbers import Number\n",
    "from pandas import DataFrame\n",
    "import sys, codecs, numpy\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train(words(file('big.txt').read()))\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in splits if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def light_process(tweet):\n",
    "    tweet = re.sub('@[^\\s]+','user',tweet)\n",
    "    tweet = tweet.replace('?', ' question ', 1)\n",
    "    tweet = tweet.replace('!', ' exclamation ', 1)\n",
    "    tweet = tweet.replace('&quot', ' ')\n",
    "\n",
    "    sad = \"\"\":-( :( :o( :[  :c( :< =[ 8( =( :{ :^( X-( </3 :_( :'( :-/ \"\"\".split()\n",
    "    sad_pattern = \"|\".join(map(re.escape, sad))\n",
    "    happy = \"\"\":-) :) :o) :] :3 :c) :> =] 8) =) :} :^)\n",
    "             :D 8-D 8D 8^P x-D xD X-D XD =-D =D =-3 =3 B^D \"\"\".split()\n",
    "    happy_pattern = \"|\".join(map(re.escape, happy))\n",
    "\n",
    "    tweet = re.sub(happy_pattern,' happy ', tweet)\n",
    "    tweet = re.sub(sad_pattern,' sad ', tweet)\n",
    "\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',tweet)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tweet = regex.sub(' ', tweet)\n",
    "    tweet = tweet.lower()\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    tweet = pattern.sub(r\"\\1\\1\", tweet)\n",
    "    tweet = re.split('(\\d+)',tweet)\n",
    "    tweet = ' '.join(tweet)\n",
    "    tweet = tweet.translate(None, digits)\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = tweet.replace('oh', ' sighn ')\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(tweet, vocab, wordlist):\n",
    "    tweet = light_process(tweet)\n",
    "    correct_words = []\n",
    "    for term in tweet.split(' '):\n",
    "        if len(term) >=3:\n",
    "            if term in vocab:\n",
    "                correct_words.append(term)\n",
    "            else:\n",
    "                term =  correct(term)\n",
    "                if term in vocab:\n",
    "                    correct_words.append(term)\n",
    "                else:\n",
    "                    term = crazy_process(term, vocab, wordlist)\n",
    "                    if term !='':\n",
    "                        correct_words.append(term)\n",
    "    return ' '.join(correct_words)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def InitializeWords():\n",
    "    wordlist = 'dic.txt' # A file containing common english words\n",
    "    content = None\n",
    "    with open(wordlist) as f:\n",
    "        content = f.readlines()\n",
    "    return [word.rstrip('\\n') for word in content]\n",
    "\n",
    "\n",
    "def ParseWord(term, wordlist):\n",
    "    words = []\n",
    "    # Remove hashtag, split by dash\n",
    "    word = FindWord(term, wordlist)    \n",
    "    while word != None and len(term) > 0:\n",
    "        words += [word]            \n",
    "        if len(term) == len(word): # Special case for when eating rest of word\n",
    "            break\n",
    "        term = term[len(word):]\n",
    "        word = FindWord(term, wordlist)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def FindWord(token, wordlist):\n",
    "    i = len(token) + 1\n",
    "    while i > 1:\n",
    "        i -= 1\n",
    "        if token[:i] in wordlist:\n",
    "            return token[:i]\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crazy_process(word, vocab, wordlist):   \n",
    "    found = []\n",
    "    word_split = ParseWord(word, wordlist)\n",
    "    for w in word_split.split(' '):\n",
    "        if len(w)>=3:\n",
    "            if w in vocab:\n",
    "                found.append(w)\n",
    "            else:\n",
    "                w =  correct(w)\n",
    "                if w in vocab:\n",
    "                    found.append(w)\n",
    "    return ' '.join(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary based on GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class autovivify_list(dict):\n",
    "        '''Pickleable class to replicate the functionality of collections.defaultdict'''\n",
    "        def __missing__(self, key):\n",
    "                value = self[key] = []\n",
    "                return value\n",
    "\n",
    "        def __add__(self, x):\n",
    "                '''Override addition for numeric types when self is empty'''\n",
    "                if not self and isinstance(x, Number):\n",
    "                        return x\n",
    "                raise ValueError\n",
    "\n",
    "        def __sub__(self, x):\n",
    "                '''Also provide subtraction method'''\n",
    "                if not self and isinstance(x, Number):\n",
    "                        return -1 * x\n",
    "                raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "        '''Read a GloVe array from sys.argv[1] and return its vectors and labels as arrays'''\n",
    "        numpy_arrays = []\n",
    "        labels_array = []\n",
    "        with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "            for c, r in enumerate(f):\n",
    "               \n",
    "                    sr = r.split()\n",
    "                    if not sr:\n",
    "                        break\n",
    "                    labels_array.append(sr[0])\n",
    "                    vec = numpy.array([float(i) for i in sr[1:]])\n",
    "                    numpy_arrays.append(vec)\n",
    "    \n",
    "\n",
    "                    #if c == n_words:\n",
    "                    #        return numpy.array( numpy_arrays ), labels_array\n",
    "\n",
    "        return numpy.array( numpy_arrays ), labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(df, lables):\n",
    "    words = lables\n",
    "    vocab = {w: idx for idx, w in enumerate(words)}\n",
    "    ivocab = {idx: w for idx, w in enumerate(words)}\n",
    "    \n",
    "    W =  df\n",
    "    # normalize each word vector to unit variance\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "    return (W_norm, vocab, ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_vector_file = 'glove.6B.200d.txt' \n",
    "df, labels_array  = build_word_vector_matrix(input_vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, vocab, ivocab = generate(df, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000    &quot;I can taste you on my lips and smell you...\n",
       "10001    &quot;I did not have sexual relations with tha...\n",
       "10002    &quot;I do it my way, I shit on folks the oppo...\n",
       "10003    &quot;I do not like plastic!! grr! so silly an...\n",
       "10004    &quot;I don't have the strength to stay away f...\n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['SentimentText'][10000:10005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user here is the tweet we try filter correct exclamation is it working question happy for detailes go to github deeplearningcourse bigfridayday finalpresentationday'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '@team_№? 123HeRE7 iS the $*($# &quottweet&quot we try2filter, correct ! Is it working? :). For detailes go to github:  #deeplearningcourse #bigfridayday #finalpresentationday'\n",
    "text = light_process(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user here the tweet try filter correct exclamation working question happy for details github deep learning course big friday day final presentation day'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = InitializeWords()\n",
    "process(text, vocab, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_dataset(dset, voc):\n",
    "    tweets = dset\n",
    "    wordlist = InitializeWords()\n",
    "    list_ = []\n",
    "    for tweet in tweets:\n",
    "        t = process(tweet, voc, wordlist)\n",
    "        list_.append(t)\n",
    "    return list_\n",
    "\n",
    "def multi_run_wrapper(args):\n",
    "    return filter_dataset(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197300\n"
     ]
    }
   ],
   "source": [
    "dset1 = pd.read_csv(\"/home/user/Desktop/ds/ds1.csv\", delimiter=',')\n",
    "dset2 = pd.read_csv(\"/home/user/Desktop/ds/ds2.csv\", delimiter=',')\n",
    "dset3 = pd.read_csv(\"/home/user/Desktop/ds/ds3.csv\", delimiter=',')\n",
    "dset4 = pd.read_csv(\"/home/user/Desktop/ds/ds4.csv\", delimiter=',')\n",
    "dset5 = pd.read_csv(\"/home/user/Desktop/ds/ds5.csv\", delimiter=',')\n",
    "dset6 = pd.read_csv(\"/home/user/Desktop/ds/ds6.csv\", delimiter=',')\n",
    "dset7 = pd.read_csv(\"/home/user/Desktop/ds/ds7.csv\", delimiter=',')\n",
    "dset8 = pd.read_csv(\"/home/user/Desktop/ds/ds8.csv\", delimiter=',')\n",
    "print len(dset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-05-24 19:24:18.357879\n",
      "1:29:45.301593\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "p = Pool(9)\n",
    "t1 = datetime.datetime.now()\n",
    "print t1\n",
    "results = p.map(multi_run_wrapper,[(dset1['SentimentText'], vocab),\n",
    "                                   (dset2['SentimentText'], vocab),\n",
    "                                   (dset3['SentimentText'], vocab),\n",
    "                                   (dset4['SentimentText'], vocab),\n",
    "                                   (dset5['SentimentText'], vocab),\n",
    "                                   (dset6['SentimentText'], vocab),\n",
    "                                   (dset7['SentimentText'], vocab),\n",
    "                                   (dset8['SentimentText'], vocab)\n",
    "                                  ]\n",
    "               )\n",
    "print datetime.datetime.now() - t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list__ = []\n",
    "for a in range(8):\n",
    "    list__ = list__ + results[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my first bday without my grandpa '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['SentimentText'][1500123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first day without grandpa'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list__[1500123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1578625\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': list__, 'sentiment': dataset['Sentiment']})\n",
    "print(len(df))\n",
    "df.to_csv(\"cleaned_tweet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1578625 entries, 0 to 1578624\n",
      "Data columns (total 2 columns):\n",
      "sentiment    1578625 non-null int64\n",
      "text         1578564 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_tweet.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "billy brag nay worries shame though would been giggle are there layby wide enough for awning bev\n"
     ]
    }
   ],
   "source": [
    "#t = 'getthismessage ololo hello 32641try6to6process KORREkt'\n",
    "t = 'billybragsters Nay worries, shame though would of been a giggle. Are there no layby\\'s wide enough for an awning in Bev?'\n",
    "print prosess(t, vocab, wordlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
